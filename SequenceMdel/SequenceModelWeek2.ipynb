{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Word Embeddings\n",
    "## Word Representation\n",
    "- 1-hot representation\n",
    "\n",
    "V = [a, aaron, ...., zulu, <UNK>]\n",
    "\n",
    "- featurized representation\n",
    "weighted with categorical values\n",
    "\n",
    "- Visualizaing word embeddings with t-SNE or ([umap](https://github.com/lmcinnes/umap/issues))\n",
    " characteristics of a word is emobbeded into lower dimensional space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using word Embeddings\n",
    "- Named entity recognition example\n",
    "\n",
    "## Transfer learning and word embeddings\n",
    "- 1 Learn word embeddings from large text corpus (1- 100B words)\n",
    "    - or download pre-trained datasets\n",
    "- 2 Transfer embedding to new task with smaller training set (say, 100k words)\n",
    "- 3 Optional: Continue to finetune the word embeddings with new data.\n",
    "\n",
    "Face encoding is a kind of embedding, embedding characteristics of face into vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Properties of  word embedding\n",
    "- Analogies using word vectors\n",
    "\\begin{eqnarray}\n",
    "e_{man} - e_{woman} \\approx e_{king} - e_{woman} \n",
    "\\end{eqnarray}\n",
    "[Mikolov et al., 2013, Linguistic regularities in continuous space word representations](https://www.aclweb.org/anthology/N13-1090)\n",
    "\n",
    "- Cosine similarity\n",
    "\\begin{eqnarray}\n",
    "    sim(e_w, e_{king}-e_{man}+e_{woman})\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding matrix\n",
    "matrix E ($ word \\times category $)\n",
    "- In practice, use specialized function to look up an embedding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Word Embeddings: word2vec & GloVe\n",
    "## Learning word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural language model\n",
    "[Bengio et al., 2003, A neural probabilistic language model](http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)\n",
    "\n",
    "Other context/target pairs, use last 4 words to be 'context' and the word would be 'target'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2vec\n",
    "- Skipgrams [Mikolov et al., 2013, Efficient estimation of word representations in vector space](https://arxiv.org/abs/1301.3781)\n",
    "- Model\n",
    "Vocab size = 10,000k\n",
    "\n",
    "- Problems with softmax classification\n",
    "\\begin{eqnarray}\n",
    "    p(t|c) = \\frac{e^{\\theta_t^T e_c}}{\\sum_{j=1}^{10,000}e^{\\theta_j^T e_c}}\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Negative Sampling\n",
    "Defining a new learning problem [Mikolov et al., 2013. Disstributed  representation of words and phrases and compositionality](https://arxiv.org/abs/1310.4546)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GloVe word vectors\n",
    "- Glove: global vectors wor word representation\n",
    "[Pennington et al., 2014. GloVE: Global vectors for word representation](https://nlp.stanford.edu/pubs/glove.pdf)\n",
    "\n",
    "minimize $\\sum_{i=1}^{10,000}\\sum_{j=1}^{10,000} f(X_{ij}) (\\theta_i^T e_j + b_i - b_{j'} - \\log X_{ij})^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applications using Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Classification\n",
    "Many-to-one problem of RNN sentiment classification.\n",
    "'completely lacking in good services, good foods.'\n",
    "This sentence has a lot of words 'good', but the sentence meant to say 'pretty bad'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debiasing word embeddings\n",
    "### The problem of bias in word embeddings\n",
    "Man:Woman as King:Queen\n",
    "Man:Computer_programmer as Woman:Homemaker -> horrable mistakes Mother\n",
    "Father:Doctor as Mother:Nurse mistakes (x)\n",
    "Word embeddings can reflect gender, ethnicisty, age, sextual, orientation, and other biases of the text used to train the model.\n",
    "\n",
    "[Bolukbasi et al., 2016. Man is to computer programmer as woman is to homemaker?](https://arxiv.org/abs/1607.06520)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Addressing bias in word embeddings\n",
    "- 1 Identify bias direction\n",
    "- 2 Neutralize: For every word that is not definitional, project to get rid of bias\n",
    "- 3 Equalize pairs\n",
    "How can we define what is the bias??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice questions\n",
    "## Q1 Suppose you learn a word embedding for a vocabulary of 10000 words. Then the embedding vectors should be 10000 dimensional, so as to capture the full range of variation and meaning in those words.\n",
    "## Answer: False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2 What is t-SNE?\n",
    "- A linear transformation that allows us to solve analogies on word vectors\n",
    "- A non-linear dimensionality reduction technique\n",
    "- A supervised learning algorithm for learning word embeddings\n",
    "- An open-source sequence modeling library\n",
    "## Answer: A non-linear dimensionality reduction technique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3 Suppose you download a pre-trained word embedding which has been trained on a huge corpus of text. You then use this word embedding to train an RNN for a language task of recognizing if someone is happy from a short snippet of text, using a small training set.\n",
    "\n",
    "Then even if the word “ecstatic” does not appear in your small training set, your RNN might reasonably be expected to recognize “I’m ecstatic” as deserving a label $y=1$.\n",
    "\n",
    "### Answer: True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4 Which of these equations do you think should hold for a good word embedding? (Check all that apply)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q5 Let $E$ be an embedding matrix, and let $e_{1234}$ be a one-hot vector corresponding to word 1234. Then to get the embedding of word 1234, why don’t we call $E\\times e_{1234}$ in Python?\n",
    "### Answer: It is computationally wasteful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q6 When learning word embeddings, we create an artificial task of estimating $P(target|context)$. It is okay if we do poorly on this artificial prediction task; the more important by-product of this task is that we learn a useful set of word embeddings.\n",
    "### Answer: True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q7 In the word2vec algorithm, you estimate $P(t|c)$, where $t$ is the target word and $c$ is a context word. How are $t$ and $c$ chosen from the training set? Pick the best answer.\n",
    "- $c$ is a sequence of several words immediately before $t$.\n",
    "- $c$ is the sequence of all the words in the sentence before $t$.\n",
    "- $c$ and $t$ are chosen to be nearby words.\n",
    "- $c$ is the one word that comes immediately before $t$.\n",
    "### Answer: $c$ and $t$ are chosen to be nearby words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q8 Suppose you have a 10000 word vocabulary, and are learning 500-dimensional word embeddings. The word2vec model uses the following softmax function:\n",
    "\\begin{eqnarray}\n",
    "    p(t|c) = \\frac{e^{\\theta_t^T e_c}}{\\sum_{j=1}^{1,000}e^{\\theta_j^T e_c}}\n",
    "\\end{eqnarray}\n",
    "Which of these statements are correct? Check all that apply.\n",
    "\n",
    "- θt and ec are both 500 dimensional vectors.\n",
    "- θt and ec are both 10000 dimensional vectors.\n",
    "- θt and ec are both trained with an optimization algorithm such as Adam or gradient descent.\n",
    "- After training, we should expect θt to be very close to ec when t and c are the same word.\n",
    "### Answer:\n",
    "- θt and ec are both 500 dimensional vectors.\n",
    "- θt and ec are both trained with an optimization algorithm such as Adam or gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q9 Suppose you have a 10000 word vocabulary, and are learning 500-dimensional word embeddings.The GloVe model minimizes this objective:\n",
    "Which of these statements are correct? Check all that apply.\n",
    "\n",
    "minimize $\\sum_{i=1}^{10,000}\\sum_{j=1}^{10,000} f(X_{ij}) (\\theta_i^T e_j + b_i - b_{j'} - \\log X_{ij})^2$\n",
    "\n",
    "- θi and ej should be initialized to 0 at the beginning of training.\n",
    "- θi and ej should be initialized randomly at the beginning of training.\n",
    "- Xij is the number of times word i appears in the context of word j.\n",
    "- The weighting function f(.) must satisfy f(0)=0\n",
    "\n",
    "### Answer: \n",
    "- θi and ej should be initialized randomly at the beginning of training.\n",
    "- Xij is the number of times word i appears in the context of word j.\n",
    "- The weighting function f(.) must satisfy f(0)=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q10  Question 10 You have trained word embeddings using a text dataset of m1 words. You are considering using these word embeddings for a language task, for which you have a separate labeled dataset of m2 words. Keeping in mind that using word embeddings is a form of transfer learning, under which of these circumstance would you expect the word embeddings to be helpful?\n",
    "### Answer: $m_1 >> m_2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
